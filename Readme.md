# Image Alignment OMR Toolkit

Computer-vision utility for turning imperfect OMR (Optical Mark Recognition) scans into neatly aligned canvases. The workflow builds a YOLOv8 detector that understands each answer block, crops those pieces from an incoming scan, and pastes them onto a clean template so downstream grading software can read them reliably.

## How the Pipeline Fits Together

1. **Convert PDFs to images** – Use `pdf-jpg/pdf.py` to batch turn each multi-page PDF into a single high-resolution JPG so YOLO can read it.
2. **Label & configure the dataset** – `dataset/data.yaml` plus the `dataset/images` and `dataset/labels` folders follow Ultralytics' expected structure (YOLO text labels with `[class, x_center, y_center, width, height]`).
3. **Train the detector** – `train.py` fine-tunes `yolov8n.pt` on the Mac M-series GPU (with MPS fallbacks) to learn where every OMR block lives.
4. **Inspect detections** – `test.py` draws colored boxes on `template_omr.png` to visually verify that YOLO predictions line up with the template.
5. **Align a new scan** – `main.py` loads the trained weights, runs inference on the latest scan, and pastes each detected block onto a blank canvas using your master coordinates. Results land under `test/results` as `final_blend*.jpg`.

## Repository Tour (Plain English)

| Path | What it does |
| --- | --- |
| `main.py` | End-to-end alignment script: loads the YOLO model, detects each block on the first page of a PDF, and pastes those crops onto a clean 114 DPI canvas using the hard-coded `MASTER_COORDS` dictionary. |
| `train.py` | YOLOv8 training entry point. Handles Apple Silicon quirks (MPS fallback, disabled AMP, smaller batch) and writes the resulting model to `runs/detect/train/weights/best.pt`. |
| `test.py` | Quick visualization helper that runs the trained detector on `template_omr.png`, draws prediction boxes, and saves/displays `omr_structure_diagram.jpg`. Useful for sanity-checking labels. |
| `pdf-jpg/pdf.py` | Batch utility that converts the first page of every PDF in a folder into sequential JPGs (`image_01.jpg`, `image_02.jpg`, …). Keeps logs so you can spot failed conversions. |
| `dataset/data.yaml` | YOLO dataset spec: points to `images/train`, `images/val`, and names the five detector classes (`ans_1`, `ans_2`, `ans_3`, `mobile_number`, `program_id`). |
| `dataset/images/` | Training and validation imagery (converted scans). Mirrors Ultralytics' default folder names. |
| `dataset/labels/` | YOLO-format label text files that pair with each image. Every line is the normalized bounding box for a single bubble group. |
| `runs/detect/train/` | Auto-generated by Ultralytics. Contains training curves, metrics, and the model checkpoints (`weights/best.pt`, `weights/last.pt`) used everywhere else. |
| `test/` | Raw PDFs for manual experimentation plus `test/results/final_blend*.jpg`, which are example aligned outputs produced by `main.py`. |
| `yolov8n.pt` | The base Ultralytics nano weights you fine-tune. Small enough to train quickly on the M4/M3 GPUs. |
| `omr_structure_diagram.jpg` | Latest structural visualization emitted by `test.py`, showing how YOLO sees the template layout. |
| `requirements.txt` | Locked dependency list (Ultralytics, OpenCV, pdf2image, PyTorch 2.9.1, etc.). Install with `pip install -r requirements.txt`. |
| `runs/weghts/best.pt`, is the trained model
 that was trained on the dataset and is used by `main.py` for inference.

## Quick Start

1. **Install dependencies**
	```bash
	python3 -m venv venv && source venv/bin/activate
	pip install --upgrade pip
	pip install -r requirements.txt
	```
2. **Prepare images**
	```bash
	python pdf-jpg/pdf.py
	```
	Update `input_folder`/`output_folder` at the bottom of the script to match your files.
3. **Label data** using a tool such as LabelImg (already listed in `requirements.txt`). Save YOLO-format TXT files into `dataset/labels/train` and `dataset/labels/val`.
4. **Train YOLO**
	```bash
	python train.py
	```
	Adjust `epochs`, `imgsz`, or `device` if you move to a different GPU/CPU setup.
5. **Visualize detections (optional)**
	```bash
	python test.py
	```
6. **Align a new scan**
	```bash
	python main.py
	```
	Make sure `align_omr_to_canvas` points to the correct PDF, model weights, and output filename.

## Adapting the Template

- Update `MASTER_COORDS` in `main.py` if your answer blocks shift on the canvas. Coordinates represent the **top-left pixel** for each pasted crop on a blank 940×1329 (8.243 in × 11.66 in @ 114 DPI) canvas.
- If your PDF has multiple pages, change the `convert_from_path` arguments so you align each page separately.
- For higher fidelity outputs, raise the inference DPI in `main.py` and the training `imgsz` in `train.py`, but keep them in sync.

## Troubleshooting Tips

- **MPS / Apple Silicon quirks** – The training script already disables AMP and sets `PYTORCH_ENABLE_MPS_FALLBACK=1`. Keep those settings unless PyTorch issues new fixes.
- **Shape mismatch errors** – Usually resolved by keeping `imgsz` divisible by 32 and ensuring all training labels exist for every image.
- **Blank canvas regions** – If a block does not appear in the final blend, confirm the detected class name matches a key inside `MASTER_COORDS`.
- **Color profile shifts** – `main.py` works in BGR (OpenCV). Convert to RGB only if you are exporting for web usage.

With this overview you can quickly understand which script handles each stage and customize the OMR pipeline to suit your own forms.
